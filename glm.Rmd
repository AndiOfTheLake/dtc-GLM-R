---
title: 'dtc: GLM in R'
author: "Andi"
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
knitr::read_chunk('glm.R')
```

```{r}
#```{r, echo = FALSE, results='hide'}
# if we used both 'echo=TRUE' and 'results=hide' the pipe would not work properly
# if we used 'echo = FALSE' and 'results=hide' we would have only messages (i.e. attaching package) If we don't want them we set 'error = FALSE', 'warning = FALSE', and 'message = FALSE'.
```

## Refresher on fitting linear models

```{r, chick, echo=FALSE, results='hide'}

```

```{r}
# Fit a lm()
lm(formula = weight ~ Diet, data = chick_weight_end)

# Fit a glm()
glm(formula = weight ~ Diet , data = chick_weight_end, family = 'gaussian')
```

## Fitting a Poisson regression in R

```{r, dat, echo=FALSE, results='hide'}

```

```{r}
# fit y predicted by x with data.frame dat using the poisson family
poisson_out <- glm(count ~ time, family = "poisson", data = dat)

# print the output
summary(poisson_out)
```

## Comparing linear and Poisson regression

```{r}
# Fit a glm with count predicted by time using data.frame dat and gaussian family
lm_out <- glm(count ~ time, data = dat, family = "gaussian")

summary(lm_out)
summary(poisson_out)
```

## Intercepts-comparisons versus means

```{r, scores, echo=FALSE, results='hide'}

```

```{r}
# Fit a glm() that estimates the difference between players
summary(glm(goal ~ player, data = scores, family = 'poisson'))

# Fit a glm() that estimates an intercept for each player 
summary(glm(goal ~ player - 1, data = scores, family = 'poisson'))
```

Note that the intercept from model 1, (Intercept), is the same as Lou's coefficient from model 2, playerLou. R allows for two different types of intercepts to be estimated. The first and default setting estimated the number of goals scored by Sam and the difference in goals scored by Lou. The second setting, using - 1, estimated the number of goals scored by Sam AND the number of goals scored by Lou. More generally, the default formula in R uses the first factor level and contrasts all other levels to it. Reording a factor allows you to change the reference level.

## Applying summary(), print(), and tidy() to glm

```{r, fire, echo=FALSE, results='hide'}

```

```{r}
# Build your linear and Poisson regression models
lm_out <- lm(Number ~ Month, data = dat)
poisson_out <- glm(Number ~ Month, data = dat, family = 'poisson')

# Examine the outputs using print()
print(lm_out)
print(poisson_out)

# Examine the outputs using summary()
summary(lm_out)
summary(poisson_out)

# Examine the outputs using tidy()
library(broom)
tidy(lm_out)
tidy(poisson_out)
```

## Extracting coefficients from glm()

```{r}
# Extract the regression coefficients
coef(poisson_out)

# Extract the confidence intervals
confint(poisson_out)
```

## Predicting with glm()

```{r, new fire, echo=FALSE, results='hide'}

```


```{r}
# print the new input months
print(new_dat)

# use the model to predict with new data 
pred_out <- predict(object = poisson_out, newdata = new_dat, type = "response")

# print the predictions
print(pred_out)
```

## Fitting a logistic regression

```{r}
bus<-read.csv("bus.csv")
bus$Bus<-factor(bus$Bus, levels = c("No", "Yes"))

# Build a glm using the bus data frame that models Bus predicted by CommuteDays
bus_out <- glm(Bus ~CommuteDays, data = bus, family = "binomial")
```

## Examining & interpreting logistic regression outputs

```{r}
# Print the bus_out (be sure to use the print function)
print(bus_out)

# Look at the summary of bus_out
summary(bus_out)

# Look at the tidy() output of bus_out
tidy(bus_out)
```

## Simulating binary data

```{r}
set.seed(613)

# Simulate 1 draw with a sample size of 100
binomial_sim <- rbinom(n = 1, size = 100, prob = 0.5)

# Simulate 100 draw with a sample size of 1 
bernoulli_sim <- rbinom(n = 100, size = 1, prob = 0.5)

# Print the results from the binomial
print(binomial_sim)

# Sum the results from the Bernoulli
sum(bernoulli_sim)
```

## Long-form logistic regression input

directly models each observation

```{r, data long, echo=FALSE, results='hide'}

```

```{r}
data_long

# Fit a a long format logistic regression
lr_1 <- glm(y ~ x, data = data_long, family = "binomial")
print(lr_1)
```

There are 28 entries hence the degrees of freedom are 27. This is because degrees of freedom are usually the number of data points minus the number of parameters estimated.

## Wide-form input logistic regression

```{r, data wide, echo=FALSE, results='hide'}

```

```{r}
data_wide

# Fit a wide form logistic regression
lr_2 <- glm(cbind(success, fail) ~ x, family = 'binomial',
            data = data_wide)

# Print the output of lr_2
print(lr_2)

# Using dataWide, fit a glm with successProportion
# predicted by x and weights = Total
lr_3 <- glm(successProportion ~ x, weights = Total, data = data_wide, family = "binomial")

# Print the output of lr_3
print(lr_3)
```

While all three methods produce the same regression coefficients, notice how the degrees of freedom in model 1 differed from the degrees of freedom in models 2 and 3. In general, the degrees of freedom are greater for the long form input. The choice of model input formatting is a personal choice and is usually driven by the structure of the data and questions being asked with the data. 

## Fitting probits and logits

```{r}
# Fit a GLM with a logit link and save it as bus_logit
bus_logit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "logit"))

# Fit a GLM with probit link and save it as bus_probit
bus_probit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "probit"))

# Print model summaries
summary(bus_logit)
summary(bus_probit)
```

## Simulating a logit

```{r}
set.seed(613)

# Convert from the logit scale to a probability
p <- plogis(0)
p

# Simulate a logit 
rbinom(n = 10, size = 1, p)
```

## Simulating a probit

```{r}
set.seed(613)

# Convert from the probit scale to a probability
p <- pnorm(0)
p

# Simulate a probit
rbinom(n = 10, size = 1, p)
```

## lm vs. Poisson coefficients

```{r, eval=FALSE}
# Extract the coeffients from lm_out
lm_coef <- coef(lm_out)
lm_coef

# Extract the coefficients from poisson_out
poisson_coef <- coef(poisson_out)
poisson_coef

# Take the exponential using exp()
poisson_coef_exp <- exp(poisson_coef)
poisson_coef_exp
```

## Poisson regression plotting

```{r, pois plot, echo=FALSE, results='hide'}

```

```{r}
library(ggplot2)

# Use geom_smooth to plot a continuous predictor variable
ggplot(data = dat, aes(x = dose, y = cells)) + 
	geom_jitter(width = 0.05, height = 0.05) + 
	geom_smooth(method = 'glm', method.args = list(family = 'poisson'))
```

## Extracting and interpreting odds-ratios (for logistic regression)

```{r, eval=FALSE}
# Extract out the coefficients 
coef_out <- coef(bus_out)

# Convert the coefficients to odds-ratios 
exp(coef_out)
```

## Odds-ratios & confidence intervals in the Tidyverse

```{r,eval=FALSE}
# Exponentiate the results and extract the confidence intervals of bus_out with tidy()
tidy(bus_out, exponentiate = TRUE, conf.int = TRUE)
```

## Default trend lines

```{r}
bus$Bus2<-ifelse(bus$Bus=="Yes", 1, 0)
head(bus)

# Create a jittered plot of MilesOneWay vs Bus2 using the bus dataset
gg_jitter <- ggplot(data = bus, aes(x = MilesOneWay, y = Bus2)) + 
	geom_jitter(width = 0, height = 0.05) +
	ylab("Probability of riding the bus") +
	xlab("One-way commute trip (in miles)")

# Add a geom_smooth() to your plot
gg_jitter + geom_smooth()
```

This does not look reasonable! We need to use the `"glm"` method.

```{r}
# Create a jittered plot of MilesOneWay vs Bus2 using the bus dataset
gg_jitter <- ggplot(data = bus, aes(x = MilesOneWay, y = Bus2)) + 
	geom_jitter(width = 0, height = 0.05) +
	ylab("Probability of riding the bus") +
	xlab("One-way commute trip (in miles)")

# Add a geom_smooth() that uses a GLM method to your plot
gg_jitter + geom_smooth(method =  "glm" , method.args = list(family = "binomial"))
```

## Comparing probits and logits

```{r}
# Add geom_smooth() lines for the probit and logit link functions
gg_jitter + 
	geom_smooth(method = 'glm', 
                method.args = list(family = binomial(link = "probit")), 
                color = 'red', se = FALSE) +
	geom_smooth(method = 'glm', 
                method.args = list(family = binomial(link = "logit")), 
                color = 'blue', se = FALSE)
```

## Fitting a multiple logistic regression

```{r}
# Build a logistic regression with Bus predicted by CommuteDays and MilesOneWay
bus_both <- glm(Bus ~ CommuteDays + MilesOneWay, data = bus, family = "binomial")

# Look at the summary of the output
summary(bus_both)
```

## Building two models

```{r}
# Build a logistic regression with Bus predicted by CommuteDays
bus_days <- glm(Bus ~ CommuteDays, data = bus, family = "binomial")

# Build a logistic regression with Bus predicted by MilesOneWay
bus_miles <- glm(Bus ~ MilesOneWay, data = bus, family = "binomial")

summary(bus_days)
summary(bus_miles)
```

## Comparing variable order

The order of predictor variables can be important, especially if predictors are correlated. This is because changing the order of correlated predictor variables can change the estimates for the regression coefficients. The name for this problem is Multicollinearity.

During this exercise, you will build two different multiple regressions with the bus data in order to compare the importance of model inputs order. First, examine the correlation between CommuteDays and MilesOneWay. Second, build two logistic regressions using the bus data frame where Bus is predicted by CommuteDays and MilesOneWay in separate orders.

After you build the two models, look at each model's summary() to see the outputs.

```{r}
# Run a correlation
cor(bus$CommuteDays, bus$MilesOneWay)

# Build a glm with CommuteDays first and MilesOneWay second
bus_one <- glm(Bus ~ CommuteDays + MilesOneWay, data = bus, family = "binomial")

# Build a glm with MilesOneWay first and CommuteDays second
bus_two <- glm(Bus ~ MilesOneWay + CommuteDays, data = bus, family = "binomial")

# Print model summaries
summary(bus_one)
summary(bus_two)
```

Notice how in this case, order was not important for the regression predictor variables. Usually, this is the case. Practically speaking, the easiest way to check this assumption is to change the order of the variables in the regression.

## Multiple slopes

```{r, eval=FALSE}
# Use model.matrix() with size
model.matrix(~ size)
model.matrix(~ size + count)
```

## Intercepts

```{r, eval=FALSE}
# Create a matrix that includes a reference intercept
model.matrix(~ color)

# Create a matrix that includes an intercept for each group
model.matrix(~ color - 1)
```

Notice how model.matrix() has different outputs for each input. The first input gives a baseline intercept and contrast. The second input gives an intercept for each group. The first allows you to compare groups to a reference group and the second allows you to compare all groups to zero.

## Multiple intercepts

```{r, eval=FALSE}
# Create a matrix that includes color and then shape  
model.matrix( ~ color + shape - 1)

# Create a matrix that includes shape and then color 
model.matrix(~ shape + color - 1)
```

Notice how order is important for the inputs of model.matrix(). The first method created intercepts for each color while the second method created intercepts for each group.

## Simpson's paradox

```{r, ucb, echo=FALSE, results='hide'}

```

```{r}
UCB_data

# Build a binomial glm where Admitted and Rejected are predicted by Gender
glm_1 <- glm(cbind(Admitted, Rejected) ~ Gender, family = 'binomial', data = UCB_data)

# Build a binomial glm where Admitted and Rejected are predicted by Gender and Dept
glm_2 <- glm(cbind(Admitted, Rejected) ~ Gender + Dept, family = 'binomial', data = UCB_data)

# Look at the summary of both models
summary(glm_1)
summary(glm_2)
```

Note that once Dept is included in the model, the effect of gender is reversed and is no longer significant.

## Include a power term

```{r}
# Plot linear effect of travel distance on probability of taking the bus
gg_jitter <-
	ggplot(data = bus, aes(x = MilesOneWay, y = Bus2)) + 
	geom_jitter(width = 0, height = 0.05) + 
	geom_smooth(method = 'glm', 
                method.args = list(family = 'binomial'))

# Add a non-linear equation to a geom_smooth()
gg_jitter +
	geom_smooth(method = 'glm', 
                method.args = list(family = 'binomial'), 
                formula = y ~ I(x^2), 
                color = 'red')
```


